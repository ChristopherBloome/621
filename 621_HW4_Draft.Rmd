---
title: 'Homework 3'
author: 'Discussion Group 1'
date: '04/09/2021'
output:
   rmdformats::readthedown
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



\newpage


```{r, message=FALSE,warning=FALSE, echo=F}
# loading libraries
library(tidyverse)
library(caret)
library(pROC)
library(knitr)
library(Amelia)
library(naniar)
library(reshape2)
library(stats)
library(corrplot)
library(e1071)
library(jtools)
library(performance)
library(rJava)
library(glmulti)
library(cvms)
library(ROCR)
library(MASS)
```


# Overview

In this homework assignment, you will explore, analyze and model a data set containing information on crime for various neighborhoods of a major city. Each record has a response variable indicating whether or not the crime rate is above the median crime rate (1) or not(0). <br>
<br>
Your objective is to build a binary logistic regression model on the training data set to predict whether the neighborhood will be at risk for high crime levels. You will provide classifications and probabilities for the evaluation data set using your binary logistic regression model. You can only use the variables given to you (or variables that you derive from the variables provided). Below is a short description of the variables of interest in the data set: <br>

- **zn**: proportion of residential land zoned for large lots (over 25000 square feet) (predictor variable)
- **indus**: proportion of non-retail business acres per suburb (predictor variable)
- **chas**: a dummy var. for whether the suburb borders the Charles River (1) or not (0) (predictor variable)
- **nox**: nitrogen oxides concentration (parts per 10 million) (predictor variable)
- **rm**: average number of rooms per dwelling (predictor variable)
- **age**: proportion of owner-occupied units built prior to 1940 (predictor variable)
- **dis**: weighted mean of distances to five Boston employment centers (predictor variable)
- **rad**: index of accessibility to radial highways (predictor variable)
- **tax**: full-value property-tax rate per $10,000 (predictor variable)
- **ptratio**: pupil-teacher ratio by town (predictor variable)
- **black**: 1000(Bk - 0.63)2 where Bk is the proportion of blacks by town (predictor variable)
- **lstat**: lower status of the population (percent) (predictor variable)
- **medv**: median value of owner-occupied homes in $1000s (predictor variable)
- **target**: whether the crime rate is above the median crime rate (1) or not (0) (response variable)   

# Data Exploration

```{r}
training <- read.csv('./crime-training-data_modified.csv')
training2 <- training # for melting and box plot
evaluation <- read.csv('./crime-evaluation-data_modified.csv')

training %>% head() %>% kable() 

# Converting to factor
var <- c("chas","target")
training[,var] <- lapply(training[,var], as.factor)
evaluation$chas <- as.factor(evaluation$chas)

```
Checking missing data

```{r, message=FALSE, warning=FALSE}
missmap(training, main="Missing Values") # using Amelia package
colSums(is.na(training))
```

```{r, message=FALSE, warning=FALSE}
# Boxplot to see distributions with target variable
melt(training2, id.vars='target') %>% mutate(target = as.factor(target)) %>% 
  ggplot(., aes(x=variable, y=value))+geom_boxplot(aes(fill=target))+facet_wrap(~variable, dir='h',scales='free')+ labs(title="BoxPlot - Predictors Data Distribution with Target Variable")

# Correlation matrix among variables
training2 %>% 
  cor(., use = "complete.obs") %>%
  corrplot(., method = "color", type = "upper", tl.col = "black", tl.cex=.8, diag = FALSE)

# Correlation table 
correlation <- training2 %>% 
  cor(., use = "complete.obs") %>%
  as.data.frame() %>%
  rownames_to_column()%>%
  gather(Variable, Correlation, -rowname) 

correlation %>%
  filter(Variable == "target") %>%
     arrange(desc(Correlation)) %>%
  kable() 

# Density plot to check normality
melt(training2, id.vars='target') %>% mutate(target = as.factor(target)) %>% 
  ggplot(., aes(x=value))+geom_density(fill='gray')+facet_wrap(~variable, scales='free')+
  labs(title="Density Plot for Normality and Skewness") + 
  theme_classic()

# Skewness and outliers
sapply(training2, skewness, function(x) skewness(x))
```

# Data Preparation

## Data Splitting

```{r, message=FALSE, warning=FALSE}
# Data splitting into train and test datasets out of training2
set.seed(1003)
training_partition <- createDataPartition(training2$target, p=0.7, list = FALSE, times=1)
train2 <- training2[training_partition, ]
test2 <- training2[-training_partition, ]

sapply(training2, skewness, function(x) skewness(x))
```

## log transformation

```{r, warning=FALSE, message=FALSE}
train_log <- train2 # copy of basic model for log transformation
test_log <- test2


train_log$zn <- log10(train_log$zn + 1)
test_log$zn <- log10(test_log$zn + 1)

# Plot and check skewness
sapply(train_log, skewness, function(x) skewness(x))
ggplot(melt(train_log), aes(x=value))+geom_density()+facet_wrap(~variable, scales='free') + labs(title="Log Transformation")

```

## BoxCox Transformation

```{r, message=FALSE, warning=FALSE}
# Copy of train and test
train_boxcox <- train2
test_boxcox <- test2

# Preprocessing
preproc_value <- preProcess(train2[,-1] , c("BoxCox", "center", "scale"))

# Transformation on both train and test datasets
train_boxcox_transformed <- predict(preproc_value, train_boxcox)
test_boxcox_transformed <- predict(preproc_value, test_boxcox)

ggplot(melt(train_boxcox_transformed), aes(x=value))+geom_density()+facet_wrap(~variable, scales='free') + labs(title="BoxCox Transformation")
sapply(train_boxcox_transformed, function(x) skewness(x))
```


# Build Models

## Model 1 - Glmulti
**Model by Forhad Akbar** 
```{r}
# Copying test and train subset to unique variable name 
test_M1 <- test2
train_M1 <- train2
```

```{r, message=FALSE, warning=FALSE}
# Model1 using glmulti()
model1 <- glmulti(target ~ ., data = train_M1, level = 1, method="h", crit = "aic", plotty = FALSE, fitfunction = "glm", family=binomial)
```

```{r, message=FALSE, warning=FALSE}
summary(model1@objects[[1]]) 
```

```{r}

# Confirm I used correct model
# Ask about Target being factor earlier


test_M1$predictions<- predict(model1@objects[[1]], test_M1, type="response")
test_M1$predicted =  as.factor(ifelse(test_M1$predictions >= 0.5, 1, 0))

test_M1$target <- as.factor(test_M1$target)
confusionMatrix(test_M1$predicted, test_M1$target, positive = '1')
proc = roc(test_M1$target, test_M1$predictions)
plot(proc)
print(proc$auc)



```


## Model 2 - Stepwise Regression and Calculated Variables 
**Model by Adam Gersowitz**

 

```{r}
# Copying test and train subset to unique variable name 
train_M2 <- train2
test_M2 <- test2
```


```{r, warning=FALSE}

# Calcuated vars on test set

test_M2$target <- as.factor(test_M2$target)
test_M2$cfas <- as.factor(test_M2$chas)
test_M2$business<-test_M2$tax*(1-test_M2$indus)
test_M2$apartment<-test_M2$rm/test_M2$tax
test_M2$pollution<-test_M2$nox*test_M2$indus
test_M2$zndum <- ifelse(test_M2$zn>0,1,0)
test_M2$rmlog<-log(test_M2$rm)
test_M2$raddum <- ifelse(test_M2$rad>23,1,0)
test_M2$lstatptratio<-((1-test_M2$lstat)*test_M2$ptratio)#/test_M2$rm

# Calcuated vars on test set

train_M2$target <- as.factor(train_M2$target)
train_M2$cfas <- as.factor(train_M2$chas)
train_M2$business<-train_M2$tax*(1-train_M2$indus)
train_M2$apartment<-train_M2$rm/train_M2$tax
train_M2$pollution<-train_M2$nox*train_M2$indus
train_M2$zndum <- ifelse(train_M2$zn>0,1,0)
train_M2$rmlog<-log(train_M2$rm)
train_M2$raddum <- ifelse(train_M2$rad>23,1,0)
train_M2$lstatptratio<-((1-train_M2$lstat)*train_M2$ptratio)#/train_M2$rm

Model_2 <- glm(target~., data = train_M2, family = "binomial") %>%
  stepAIC(trace = FALSE)
summary(Model_2)




test_M2$predictions<-predict(Model_2, test_M2, type="response")
test_M2$predicted =  as.factor(ifelse(test_M2$predictions >= 0.5, 1, 0))

confusionMatrix(test_M2$predicted, test_M2$target, positive = '1')
proc = roc(test_M2$target, test_M2$predictions)
plot(proc)
print(proc$auc)



```
## Model 3 - Lasso
**Model by David Blumenstiel**

```{r}
# Copying test and train subset to unique variable name 
test_M3 <- test2
train_M3 <- train2

test_M3$target <- as.factor(test_M3$target)
train_M3$target <- as.factor(train_M3$target)

```

```{r}

library(glmnet)  #Was a helpful guide: https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html
#Data prep.  Needs to be in matrix format
#Took code from here: https://stackoverflow.com/questions/35437411/error-in-predict-glmnet-function-not-yet-implemented-method

trainx = model.matrix(~.-target,data=train_M3)     
newx = model.matrix(~.-target,data=test_M3)
#Makes a series of crossvalidated glmnet models for 100 lambda values (default)
#lamba values are constants that define coefficient shrinkage.  
glmnetmodel <- cv.glmnet(x = trainx,   #Predictor variables
                      y = train_M3[,names(train_M3) == "target"],   #Responce variable
                      family = "binomial", #Has it do logistic regression
                      nfolds = 10, #10 fold cv
                      type.measure = "class",  #uses missclassification error as loss
                      gamma = seq(0,1,0.1),  #Values to use for relaxed fit
                      relax = TRUE,#Mixes relaxed fit with regluarized fit
                      alpha = 1) #Basically a choice betwen lasso, ridge, or elasticnet regression.  Alpha = 1 is lasso.
#Predicts the probability that the target variable is 1
predictions <- predict(glmnetmodel, newx = newx, type = "response", s=glmnetmodel$lambda.min) #setting lambda.min uses the lambda value with the minimum mean cv error (picks the best model)
#Print's the coefficients the model uses
print(coef.glmnet(glmnetmodel, s = glmnetmodel$lambda.min))

```

```{r}
confusionMatrix(as.factor(ifelse(predictions >= 0.5, 1, 0)), test_M3$target, positive = '1')
proc = roc(test_M3$target, predictions)
plot(proc)
print(proc$auc)



```

## Model Selection

As Model 2 preformed the best ... 


# Rerun model on entire training set

```{r, warning=FALSE}
evaluation_F <- evaluation
training_F <- training

evaluation_F$cfas <- as.factor(evaluation_F$chas)
evaluation_F$business<-evaluation_F$tax*(1-evaluation_F$indus)
evaluation_F$apartment<-evaluation_F$rm/evaluation_F$tax
evaluation_F$pollution<-evaluation_F$nox*evaluation_F$indus
evaluation_F$zndum <- ifelse(evaluation_F$zn>0,1,0)
evaluation_F$rmlog<-log(evaluation_F$rm)
evaluation_F$raddum <- ifelse(evaluation_F$rad>23,1,0)
evaluation_F$lstatptratio<-((1-evaluation_F$lstat)*evaluation_F$ptratio)#/evaluation_F$rm

training_F$target <- as.factor(training_F$target)
training_F$cfas <- as.factor(training_F$chas)
training_F$business<-training_F$tax*(1-training_F$indus)
training_F$apartment<-training_F$rm/training_F$tax
training_F$pollution<-training_F$nox*training_F$indus
training_F$zndum <- ifelse(training_F$zn>0,1,0)
training_F$rmlog<-log(training_F$rm)
training_F$raddum <- ifelse(training_F$rad>23,1,0)
training_F$lstatptratio<-((1-training_F$lstat)*training_F$ptratio)#/training_F$rm


Model_F <- glm(target~., data = training_F, family = "binomial") %>%
  stepAIC(trace = FALSE)
summary(Model_F)


```

## Predict Test Set / Export results 

```{r}
predicted_probability <- predict(Model_F, evaluation_F, type = "response")
predicted_class <-  as.factor(ifelse(predicted_probability >= 0.5, 1, 0))
predictions <- data.frame(predicted_class,predicted_probability)
colnames(predictions) <- c("predicted_class","predicted_probability")
#write.csv(predictions, file = "predictions.csv")

```



